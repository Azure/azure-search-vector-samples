{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search with Azure AI Vision multimodal embeddings for text-to-image and image-to-image queries\n",
    "\n",
    "As a scenario, this code shows you an approach for text-to-image and image-to-image vector queries. The multimodal embeddings used in this sample are provided by [Azure AI Vision 4.0](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/image-retrieval) and the [Image Retrieval REST API](https://learn.microsoft.com/rest/api/computervision/image-retrieval) which supports built-in vectorization of images. \n",
    "\n",
    "For indexing, the pattern uses the built-in [Azure AI Vision multimodal embeddings skill](https://learn.microsoft.com/azure/search/cognitive-search-skill-vision-vectorize) to call the Image Retrieval API. Provisioning of the search service, Azure AI multi-service resource, and indexer is fully automated and included as a step in this notebook.\n",
    "\n",
    "The Azure AI multi-service resource is also used during queries to back the [vectorizer](https://learn.microsoft.com/azure/search/vector-search-how-to-configure-vectorizer). A vectorizer specifies which embedding model to use for encoding a text query string or an image. As always, it's strongly recommended that you use the same embedding model used for document vectorization during indexing and string vectorization during query processing.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "+ [Azure AI Search](https://learn.microsoft.com/azure/search/search-create-service-portal), any region and tier, but we recommend Basic or higher for this workload.\n",
    "\n",
    "+ [Azure Blob storage](https://learn.microsoft.com/azure/storage/common/storage-account-create), used as the data source during indexing. The script loads local text and images into a container.\n",
    "\n",
    "+ [Azure AI multi-service resource](https://learn.microsoft.com/azure/ai-services/multi-service-resource), used for multi-modal embeddings.\n",
    "\n",
    "+ [azd](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd), used to deploy an Azure function app and Azure resources.\n",
    "\n",
    "We use the [Azure Python SDK](https://learn.microsoft.com/python/api/azure-search-documents/?view=azure-python-preview) for indexer-driven indexing and vector query operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r multimodal-embeddings-requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision the sample\n",
    "\n",
    "This sample uses [`azd`](https://learn.microsoft.com/azure/developer/azure-developer-cli/), a bicep template, and a custom post-provision hook to provision the sample. The sample uses role-based authentication using your identity for authentication, and does not use any API keys.\n",
    "\n",
    "1. Open a PowerShell command prompt in the multimodal-embeddings folder.\n",
    "\n",
    "1. Run `azd config set defaults.subscription <yourSubscriptionID>` to set the subscription if you have multiple Azure subscriptions.\n",
    "1. Run `azd env new <your-environment-name>` to create an environment for deploying the sample.\n",
    "1. If you want to use an existing resource, set the corresponding `azd` environment variables before deployment:\n",
    "   1. Existing Search resource:\n",
    "      1. `azd env set AZURE_SEARCH_SERVICE <your-search-service-name>`\n",
    "      1. `azd env set AZURE_SEARCH_SERVICE_LOCATION <your-search-service-location>`\n",
    "      1. `azd env set AZURE_SEARCH_SERVICE_RESOURCE_GROUP <your-search-service-resource-group>`\n",
    "      1. `azd env set AZURE_SEARCH_SERVICE_SKU <your-search-service-sku>`\n",
    "      1. `azd env set AZURE_SEARCH_SERVICE_SEMANTIC_RANKER <your-semantic-ranker-sku>`\n",
    "   1. Existing AI Services resource:\n",
    "      1. `azd env set AZURE_AI_SERVICES_ACCOUNT <your-ai-services-account-name>`\n",
    "      1. `azd env set AZURE_AI_SERVICES_LOCATION <your-ai-services-account-location>`\n",
    "      1. `azd env set AZURE_AI_SERVICES_RESOURCE_GROUP <your-ai-services-resource-group>`\n",
    "   1. Existing Storage resource:\n",
    "      1. `azd env set AZURE_STORAGE_ACCOUNT <your-storage-account-name>`\n",
    "      1. `azd env set AZURE_STORAGE_ACCOUNT_LOCATION <your-storage-account-location>`\n",
    "      1. `azd env set AZURE_STORAGE_ACCOUNT_RESOURCE_GROUP <your-storage-account-resource-group>`\n",
    "1. Run `azd provision`.\n",
    "   1. Enter a region for the sample deployment. Choose a region that provides the Image Retrieval API: `East US`, `France Central`, `Korea Central`, `North Europe`, `Southeast Asia`, `West Europe`, `West US`.\n",
    "\n",
    "This step takes several minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve environment variables after provisioning\n",
    "\n",
    "The included `azd` bicep template saves all required environment variables for the notebook automatically. You can run `azd env get-values` to show the current variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all environment variables from the azd deployment\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "result = subprocess.run([\"azd\", \"env\", \"get-values\"], stdout=subprocess.PIPE, cwd=os.getcwd())\n",
    "load_dotenv(override=True, stream=StringIO(result.stdout.decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the indexer has completed successfully\n",
    "\n",
    "An indexer created in [scripts/setup.py](./scripts/setup.py) runs in the background to process all the sample images. Validate that it has completed without any errors before trying to search the sample index. It's possible the indexer may fail due to being throttled by the image embeddings from AI Services. If this is the case, try rerunning the indexer in the Azure portal to resolve the issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "search_indexer_client = SearchIndexerClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"), credential=DefaultAzureCredential())\n",
    "status = search_indexer_client.get_indexer_status(name=os.getenv(\"AZURE_SEARCH_INDEXER\"))\n",
    "print(f\"Status: {status.last_result.status}, Items Processed: {status.last_result.item_count}, Items Failed: {status.last_result.failed_item_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a vector search by vectorizing your text query\n",
    "\n",
    "Perform a vector search to find the most relevant images based on the text query.\n",
    "\n",
    "Vector queries call [VectorizableTextQuery](https://learn.microsoft.com/python/api/azure-search-documents/azure.search.documents.models.vectorizabletextquery) to vectorize a query text string that's used to match against vectorized images created by AI Services. VectorizeableTextQuery uses the vectorizer defined in the index, which is Azure AI Vision image retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from IPython.display import Image\n",
    "\n",
    "apples_image_directory = os.path.join('..', '..', '..', '..', 'data', 'images', 'apples')\n",
    "# Generate text embeddings for the query  \n",
    "query = \"green apple\"  \n",
    "  \n",
    "# Initialize the SearchClient  \n",
    "search_client = SearchClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"), index_name=os.getenv(\"AZURE_SEARCH_INDEX\"), credential=DefaultAzureCredential())  \n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"embedding\")  \n",
    "\n",
    "# Perform vector search  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"metadata_storage_path\"],\n",
    "    top=1\n",
    ")   \n",
    "  \n",
    "# Print the search results  \n",
    "for result in results:  \n",
    "    print(f\"Image: {os.path.basename(result['metadata_storage_path'])}\") \n",
    "    display(Image(filename=os.path.join(apples_image_directory, os.path.basename(result['metadata_storage_path'])))) \n",
    "    print(\"\\n\") \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a vector search by vectorizing your image query from a URL\n",
    "\n",
    "Perform a vector search to find the most relevant images based on the image query.\n",
    "\n",
    "Vector queries call [VectorizableImageUrlQuery](https://learn.microsoft.com/python/api/azure-search-documents/azure.search.documents.models.vectorizableimageurlquery?view=azure-python-preview) to vectorize a query image from a URL that's used to match against vectorized images created by AI Services. VectorizableImageUrlQuery uses the vectorizer defined in the index, which is Azure AI Vision image retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizableImageUrlQuery\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/a/a6/Pink_lady_and_cross_section.jpg\"\n",
    "vector_query = VectorizableImageUrlQuery(url=url, k_nearest_neighbors=1, fields=\"embedding\")  \n",
    "\n",
    "# Perform vector search  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"metadata_storage_path\"],\n",
    "    top=1\n",
    ")   \n",
    "  \n",
    "print(\"Source Image\")\n",
    "display(Image(url=url))\n",
    "# Print the search results  \n",
    "for result in results:  \n",
    "    print(f\"Image: {os.path.basename(result['metadata_storage_path'])}\") \n",
    "    display(Image(filename=os.path.join(apples_image_directory, os.path.basename(result['metadata_storage_path'])))) \n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a vector search by vectorizing your image query from a file\n",
    "\n",
    "Perform a vector search to find the most relevant images based on the image query.\n",
    "\n",
    "Vector queries call [VectorizableImageBinaryQuery](https://learn.microsoft.com/en-us/python/api/azure-search-documents/azure.search.documents.models.vectorizableimagebinaryquery?view=azure-python-preview) to vectorize a query image from a file that's used to match against vectorized images created by AI Services. VectorizableImageBinaryQuery uses the vectorizer defined in the index, which is Azure AI Vision image retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizableImageBinaryQuery\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/c/c1/Fuji_apple.jpg\"\n",
    "response = requests.get(url)\n",
    "base64_content = base64.b64encode(response.content).decode(\"utf-8\")\n",
    "vector_query = VectorizableImageBinaryQuery(base64_image=base64_content, k_nearest_neighbors=1, fields=\"embedding\")  \n",
    "\n",
    "# Perform vector search  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"metadata_storage_path\"],\n",
    "    top=1\n",
    ")   \n",
    "  \n",
    "print(\"Source Image\")\n",
    "display(Image(url=url))\n",
    "# Print the search results  \n",
    "for result in results:  \n",
    "    print(f\"Image: {os.path.basename(result['metadata_storage_path'])}\") \n",
    "    display(Image(filename=os.path.join(apples_image_directory, os.path.basename(result['metadata_storage_path'])))) \n",
    "    print(\"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
