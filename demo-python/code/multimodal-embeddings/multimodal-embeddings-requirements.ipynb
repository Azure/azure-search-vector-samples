{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search with Azure AI Vision multimodal embeddings for text-to-image queries\n",
    "\n",
    "As a scenario, this code shows you an approach for text-to-image vector queries. As a technical sample, it demonstrates how to call a custom embedding model for situations where you want models other an Azure OpenAI or OpenAI for vectorization. The multimodal embeddings used in this sample are provided by [Azure AI Vision 4.0](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/image-retrieval) and the [Image Retrieval REST API](https://learn.microsoft.com/rest/api/computervision/image-retrieval) which supports built-in vectorization of images. \n",
    "\n",
    "For indexing, the pattern uses a custom skill to wrap an Azure function app used to call the Image Retrieval API. Provisioning of this function app and custom skill is fully automated and included as a step in this notebook.\n",
    "\n",
    "The function app is also used during queries, as the vectorizer. A vectorizer specifies which embedding model to use for vectorizing a text query string. As always, it's strongly recommended that query vectorization is performed using the same embedding model used for document vectorization during indexing.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "+ [Azure AI Search](https://learn.microsoft.com/azure/search/search-create-service-portal), any region and tier, but we recommend Basic or higher for this workload.\n",
    "\n",
    "+ [Azure Blob storage](https://learn.microsoft.com/azure/storage/common/storage-account-create), used as the data source during indexing.\n",
    "\n",
    "+ [azd](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd), used to deploy an Azure function app and Azure AI Vision in your Azure subscription.\n",
    "\n",
    "We use the [Azure Python SDK](https://learn.microsoft.com/en-us/python/api/azure-search-documents/?view=azure-python-preview) for indexer-driven indexing and vector query operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r azure-search-vector-image-python-sample-requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .env file\n",
    "\n",
    "Copy `.env-sample` to an `.env` file in the sample folder, and update accordingly. The search service and Azure Storage account must exist, but the search index and blob container are created and loaded during code execution. Provide unique names for both the index and container. Endpoint, API key, and connection string can be found in the Azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "blob_connection_string = os.environ[\"BLOB_CONNECTION_STRING\"]\n",
    "blob_container_name = os.environ[\"BLOB_CONTAINER_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision a custom skill\n",
    "\n",
    "This sample uses [`azd`](https://learn.microsoft.com/azure/developer/azure-developer-cli/) and a bicep template to deploy an Azure function.\n",
    "\n",
    "1. Open a PowerShell command prompt in the custom-skills folder.\n",
    "\n",
    "1. Run `azd config set defaults.subscription <yourSubscriptionID>` to set the subscription if you have multiple Azure subscriptions.\n",
    "1. Run `azd up`.\n",
    "\n",
    "   1. Enter a development environment name.\n",
    "   1. Enter a region for the function app. Choose a region that provides the Image Retrieval API: `East US`, `France Central`, `Korea Central`, `North Europe`, `Southeast Asia`, `West Europe`, `West US`.\n",
    "\n",
    "If you aren't prompted for an environment or region, retry `azd up -e` to specify a new environment and region.\n",
    "\n",
    "This step takes several minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve azd environment variables after provisioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all environment variables from the azd deployment\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "result = subprocess.run(\"azd env get-values\", stdout=subprocess.PIPE, cwd=os.getcwd())\n",
    "load_dotenv(override=True, stream=StringIO(result.stdout.decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get custom skill connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.web import WebSiteManagementClient\n",
    "\n",
    "subscription_id = os.environ[\"AZURE_SUBSCRIPTION_ID\"]\n",
    "client = WebSiteManagementClient(credential=DefaultAzureCredential(), subscription_id=subscription_id)\n",
    "\n",
    "resource_group = os.environ[\"AZURE_APP_SERVICE_PLAN_RESOURCE_GROUP\"]\n",
    "function_app_name = os.environ[\"AZURE_FUNCTION_APP_NAME\"]\n",
    "function_name = \"GetImageEmbedding\"\n",
    "embedding_function = client.web_apps.get_function(resource_group_name=resource_group, name=function_app_name, function_name=function_name)\n",
    "embedding_function_keys = client.web_apps.list_function_keys(resource_group_name=resource_group, name=function_app_name, function_name=function_name)\n",
    "function_url_template = embedding_function.invoke_url_template\n",
    "function_key = embedding_function_keys.additional_properties[\"default\"]\n",
    "function_app_url=f\"{function_url_template}?code={function_key}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload image to Blob storage\n",
    "\n",
    "This step pulls sample image files from the local data/images folder. It uploads the images to an Azure storage account and container name specified as an envrionment variable. This code creates the container and uploads the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Connect to Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "apples_image_directory = os.path.join('..', '..', 'data', 'images', 'apples')\n",
    "for image_name in os.listdir(apples_image_directory):\n",
    "    if image_name.endswith(\".jpeg\"):\n",
    "        image_path = os.path.join(apples_image_directory, image_name)\n",
    "        if not container_client.get_blob_client(image_name).exists():\n",
    "            with open(image_path, \"rb\") as data:\n",
    "                container_client.upload_blob(name=image_name, data=data)\n",
    "\n",
    "print(\"Uploaded sample images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a blob data source on Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import SearchIndexerDataContainer, SearchIndexerDataSourceConnection\n",
    "# Create a data source \n",
    "ds_client = SearchIndexerClient(endpoint, credential)\n",
    "container = SearchIndexerDataContainer(name=blob_container_name)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=blob_connection_string,\n",
    "    container=container\n",
    ")\n",
    "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a skillset\n",
    "\n",
    "The skillset specifies the custom skill used during indexing to vectorize images in the blob container. The `uri` specifies the function app you deployed earlier using `azd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    WebApiSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchIndexerSkillset\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\"  \n",
    "  \n",
    "skill = WebApiSkill(  \n",
    "    description=\"Skill to generate image embeddings via a custom endpoint\",  \n",
    "    context=\"/document\",\n",
    "    http_method=\"POST\",\n",
    "    batch_size=10, # Controls how many images are sent to the custom skill at a time\n",
    "    uri=function_app_url,\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"imageUrl\", source=\"/document/metadata_storage_path\"),\n",
    "        InputFieldMappingEntry(name=\"sasToken\", source=\"/document/metadata_storage_sas_token\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"vector\", target_name=\"vector\")\n",
    "    ],\n",
    ")\n",
    "  \n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to extract image vector\",  \n",
    "    skills=[skill],  \n",
    ")\n",
    "  \n",
    "ds_client.create_or_update_skillset(skillset)  \n",
    "print(f'Skillset {skillset.name} created')  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index\n",
    "Create your search index schema and vector search configuration. Here, the function app is used again in the vectorizer definition. It's used for text-to-image queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    "    CustomVectorizer,\n",
    "    CustomWebApiParameters\n",
    ")\n",
    "\n",
    "# Create a search index  \n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)  \n",
    "fields = [  \n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),  \n",
    "    SearchField(name=\"imageUrl\", type=SearchFieldDataType.String),  \n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),  \n",
    "    SearchField(  \n",
    "        name=\"imageVector\",  \n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  \n",
    "        vector_search_dimensions=1024,  \n",
    "        vector_search_profile_name=\"myHnswProfile\",  \n",
    "    ),  \n",
    "]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(  \n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],  \n",
    "   profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\", \n",
    "            vectorizer=\"customVectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[  \n",
    "        CustomVectorizer(name=\"customVectorizer\", custom_web_api_parameters=CustomWebApiParameters(uri=function_app_url))\n",
    "    ]\n",
    ")\n",
    "  \n",
    "# Create the search index with the vector search configuration  \n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an indexer\n",
    "\n",
    "Create or update an indexer to process images and populate the search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    FieldMapping\n",
    ")\n",
    "\n",
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to process images\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,  \n",
    "    field_mappings=[  \n",
    "        FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"imageUrl\"),  \n",
    "        FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")  \n",
    "    ],  \n",
    "    output_field_mappings=[  \n",
    "        FieldMapping(source_field_name=\"/document/vector\", target_field_name=\"imageVector\")  \n",
    "    ]  \n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(endpoint, credential)  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)  \n",
    "print(f'{indexer_name} is created and running. It will be several minutes before you can run the queries.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a vector search by vectorizing your text query\n",
    "\n",
    "Perform a vector search to find the most relevant images based on the text query.\n",
    "\n",
    "Vector queries call [VectorizableTextQuery](https://learn.microsoft.com/python/api/azure-search-documents/azure.search.documents.models.vectorizabletextquery) to vectorize a query text string that's used to match against vectorized images created by the custom skill. VectorizeableTextQuery uses the vectorizer defined in the index, which is the function app that runs Azure AI Vision image retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import (\n",
    "    VectorizableTextQuery\n",
    ")\n",
    "from IPython.display import Image\n",
    "\n",
    "apples_image_directory = os.path.join('..', '..', 'data', 'images', 'apples')\n",
    "# Generate text embeddings for the query  \n",
    "query = \"green apple\"  \n",
    "  \n",
    "# Initialize the SearchClient  \n",
    "search_client = SearchClient(endpoint, index_name, credential)  \n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"imageVector\")  \n",
    "\n",
    "# Perform vector search  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"imageUrl\"],\n",
    "    top=1\n",
    ")   \n",
    "  \n",
    "# Print the search results  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Image URL: {result['imageUrl']}\") \n",
    "    display(Image(filename=os.path.join(apples_image_directory, os.path.basename(result['imageUrl'])))) \n",
    "    print(\"\\n\") \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a vector search to find the most relevant images based on the image query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Generate image embeddings for the query (for context, this is a photo of a red apple)\n",
    "query = \"https://upload.wikimedia.org/wikipedia/commons/a/a6/Pink_lady_and_cross_section.jpg\"\n",
    "\n",
    "response = requests.post(function_app_url, json={ \"values\": [ { \"recordId\": 0, \"data\": { \"imageUrl\": query, \"sasToken\": \"\" } } ] })  \n",
    "response.raise_for_status()\n",
    "vector = response.json()[\"values\"][0][\"data\"][\"vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# Initialize the SearchClient  \n",
    "search_client = SearchClient(endpoint, index_name, credential)  \n",
    "vector_query = VectorizedQuery(vector=vector, k_nearest_neighbors=1, fields=\"imageVector\")  \n",
    "\n",
    "# Perform vector search  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"imageUrl\"],\n",
    "    top=1\n",
    ")   \n",
    "\n",
    "print(\"Source Image\")\n",
    "display(Image(url=query))\n",
    "# Print the search results  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Image URL: {result['imageUrl']}\") \n",
    "    display(Image(filename=os.path.join(apples_image_directory, os.path.basename(result['imageUrl'])))) \n",
    "    print(\"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
