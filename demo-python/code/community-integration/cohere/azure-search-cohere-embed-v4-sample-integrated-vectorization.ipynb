{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search: Integrated Vectorization with Cohere embed-v-4 Skillset\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Search **integrated vectorization** with skillsets to:\n",
    "\n",
    "1. **Crack PDFs** from Azure Blob Storage (built-in document processing)\n",
    "2. **Chunk documents** using the Text Split skill\n",
    "3. **Generate embeddings** using Cohere embed-v-4 via AML skill (Microsoft Foundry)\n",
    "4. **Create chunks** using index projections (one-to-many mapping)\n",
    "5. **Search** using hybrid search with semantic ranker\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Blob Storage (PDFs)\n",
    "        |\n",
    "    Data Source\n",
    "        |\n",
    "    Indexer + Skillset\n",
    "        |-- Document Cracking (built-in PDF parsing)\n",
    "        |-- Text Split Skill (chunking)\n",
    "        |-- AzureMachineLearningSkill (Cohere embed-v-4)\n",
    "        |\n",
    "    Index Projections (one-to-many)\n",
    "        |\n",
    "    Search Index (chunks with vectors + semantic config)\n",
    "        |\n",
    "    Hybrid Search + Semantic Ranker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-search-documents azure-identity python-dotenv azure-storage-blob azure-ai-inference --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: https://farzad-srch-wcus-basic.search.windows.net\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SearchField, SearchFieldDataType,\n",
    "    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,\n",
    "    SemanticConfiguration, SemanticField, SemanticPrioritizedFields, SemanticSearch,\n",
    "    SearchIndexerDataContainer, SearchIndexerDataSourceConnection,\n",
    "    SearchIndexer, SearchIndexerSkillset, SplitSkill, AzureMachineLearningSkill,\n",
    "    InputFieldMappingEntry, OutputFieldMappingEntry,\n",
    "    SearchIndexerIndexProjection, SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters, FieldMapping,\n",
    ")\n",
    "from azure.search.documents.models import VectorizedQuery, QueryType\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "from azure.ai.inference.models import EmbeddingInputType\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Configuration\n",
    "search_endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "search_api_key = os.environ[\"AZURE_SEARCH_API_KEY\"]\n",
    "storage_connection_string = os.environ[\"AZURE_STORAGE_CONNECTION_STRING\"]\n",
    "storage_container = os.environ[\"AZURE_STORAGE_CONTAINER\"]\n",
    "inference_endpoint = os.environ[\"AZURE_INFERENCE_ENDPOINT\"]\n",
    "inference_credential = os.environ[\"AZURE_INFERENCE_CREDENTIAL\"]\n",
    "\n",
    "# Resource names\n",
    "index_name = \"cohere-skillset-demo-index\"\n",
    "data_source_name = \"cohere-skillset-demo-datasource\"\n",
    "skillset_name = \"cohere-skillset-demo-skillset\"\n",
    "indexer_name = \"cohere-skillset-demo-indexer\"\n",
    "embedding_model = \"embed-v-4-0\"\n",
    "embedding_dimensions = 1536\n",
    "\n",
    "# Initialize clients\n",
    "credential = AzureKeyCredential(search_api_key)\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)\n",
    "indexer_client = SearchIndexerClient(endpoint=search_endpoint, credential=credential)\n",
    "embeddings_client = EmbeddingsClient(endpoint=inference_endpoint, credential=AzureKeyCredential(inference_credential))\n",
    "\n",
    "print(f\"Connected to: {search_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'cohere-skillset-demo-datasource' created\n"
     ]
    }
   ],
   "source": [
    "# Create data source for Azure Blob Storage\n",
    "data_source = SearchIndexerDataSourceConnection(\n",
    "    name=data_source_name, type=\"azureblob\",\n",
    "    connection_string=storage_connection_string,\n",
    "    container=SearchIndexerDataContainer(name=storage_container)\n",
    ")\n",
    "indexer_client.create_or_update_data_source_connection(data_source)\n",
    "print(f\"Data source '{data_source_name}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'cohere-skillset-demo-index' created with 5 fields\n"
     ]
    }
   ],
   "source": [
    "# Define index fields\n",
    "fields = [\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, filterable=True, analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchField(name=\"chunk_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=embedding_dimensions, vector_search_profile_name=\"cohere-vector-profile\")\n",
    "]\n",
    "\n",
    "# Vector search with HNSW defaults\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"cohere-hnsw-config\")],\n",
    "    profiles=[VectorSearchProfile(name=\"cohere-vector-profile\", algorithm_configuration_name=\"cohere-hnsw-config\")]\n",
    ")\n",
    "\n",
    "# Semantic search\n",
    "semantic_search = SemanticSearch(configurations=[\n",
    "    SemanticConfiguration(name=\"cohere-semantic-config\", prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Create index\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f\"Index '{result.name}' created with {len(result.fields)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skillset 'cohere-skillset-demo-skillset' created\n"
     ]
    }
   ],
   "source": [
    "inference_scoring_uri = f\"{inference_endpoint}/embeddings\"\n",
    "\n",
    "# Text Split Skill - chunks documents\n",
    "split_skill = SplitSkill(\n",
    "    name=\"text-split-skill\", text_split_mode=\"pages\",\n",
    "    maximum_page_length=2000, page_overlap_length=500,\n",
    "    inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/content\")],\n",
    "    outputs=[OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")]\n",
    ")\n",
    "\n",
    "# AML Skill - generates embeddings via Azure AI Inference API\n",
    "embedding_skill = AzureMachineLearningSkill(\n",
    "    name=\"cohere-embedding-skill\",\n",
    "    scoring_uri=inference_scoring_uri,\n",
    "    authentication_key=inference_credential,\n",
    "    context=\"/document/pages/*\",\n",
    "    timeout=timedelta(seconds=60),\n",
    "    degree_of_parallelism=5,\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"input\", source=\"=[$(/document/pages/*)]\"),\n",
    "        InputFieldMappingEntry(name=\"model\", source=\"='embed-v-4-0'\")\n",
    "    ],\n",
    "    outputs=[OutputFieldMappingEntry(name=\"data\", target_name=\"embedding_response\")]\n",
    ")\n",
    "\n",
    "# Index Projections - one-to-many mapping (parent doc -> chunks)\n",
    "index_projections = SearchIndexerIndexProjection(\n",
    "    selectors=[SearchIndexerIndexProjectionSelector(\n",
    "        target_index_name=index_name, parent_key_field_name=\"parent_id\",\n",
    "        source_context=\"/document/pages/*\",\n",
    "        mappings=[\n",
    "            InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
    "            InputFieldMappingEntry(name=\"chunk_vector\", source=\"/document/pages/*/embedding_response/0/embedding\"),\n",
    "            InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\")\n",
    "        ]\n",
    "    )],\n",
    "    parameters=SearchIndexerIndexProjectionsParameters(projection_mode=\"skipIndexingParentDocuments\")\n",
    ")\n",
    "\n",
    "# Create skillset\n",
    "skillset = SearchIndexerSkillset(\n",
    "    name=skillset_name, skills=[split_skill, embedding_skill], index_projection=index_projections\n",
    ")\n",
    "indexer_client.create_or_update_skillset(skillset)\n",
    "print(f\"Skillset '{skillset_name}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Run Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer 'cohere-skillset-demo-indexer' started...\n",
      "Status: success | Docs: 0 | Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Create and run the indexer\n",
    "indexer = SearchIndexer(\n",
    "    name=indexer_name,\n",
    "    data_source_name=data_source_name,\n",
    "    target_index_name=index_name,\n",
    "    skillset_name=skillset_name,\n",
    "    field_mappings=[\n",
    "        FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"parent_id\"),\n",
    "        FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")\n",
    "    ]\n",
    ")\n",
    "indexer_client.create_or_update_indexer(indexer)\n",
    "indexer_client.run_indexer(indexer_name)\n",
    "print(f\"Indexer '{indexer_name}' started...\")\n",
    "\n",
    "# Poll for completion\n",
    "while True:\n",
    "    status = indexer_client.get_indexer_status(indexer_name)\n",
    "    if status.last_result and status.last_result.status in [\"success\", \"transientFailure\", \"persistentFailure\"]:\n",
    "        print(f\"Status: {status.last_result.status} | Docs: {status.last_result.item_count} | Errors: {status.last_result.failed_item_count}\")\n",
    "        if status.last_result.errors:\n",
    "            for e in status.last_result.errors[:3]:\n",
    "                print(f\"  Error: {e.error_message}\")\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "def hybrid_search(query: str, top: int = 5):\n",
    "    \"\"\"Hybrid search with semantic ranker using Cohere embeddings.\"\"\"\n",
    "    embedding = embeddings_client.embed(\n",
    "        input=[query], model=embedding_model, input_type=EmbeddingInputType.QUERY\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    return list(search_client.search(\n",
    "        search_text=query,\n",
    "        vector_queries=[VectorizedQuery(vector=embedding, k=top, fields=\"chunk_vector\")],\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        semantic_configuration_name=\"cohere-semantic-config\",\n",
    "        select=[\"chunk_id\", \"title\", \"chunk\"],\n",
    "        top=top\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the key trends in artificial intelligence?\n",
      "------------------------------------------------------------\n",
      "1. the-state-of-enterprise-ai_2025-report.pdf (score: 2.79)\n",
      "   Deliberate change  management  They build structures that speed organizational learning, combining  centralized governance and training with distribut...\n",
      "2. the-state-of-enterprise-ai_2025-report.pdf (score: 2.76)\n",
      "   a grounded view of how  AI is being deployed inside organizations today.    The state of enterprise AI  |  2025 Report3  01 Enterprise usage is scalin...\n",
      "3. the-state-of-enterprise-ai_2025-report.pdf (score: 2.68)\n",
      "   today, despite broad availability of these tools. Models are capable of far more than  most organizations have embedded into workflows, and this prese...\n",
      "\n",
      "Query: How is AI impacting healthcare?\n",
      "------------------------------------------------------------\n",
      "1. the-state-of-enterprise-ai_2025-report.pdf (score: 2.93)\n",
      "   savings KPI.    The state of enterprise AI  |  2025 Report21  AI adoption and business impact: case evidence  Oscar Health deployed member-facing chat...\n",
      "2. the-state-of-enterprise-ai_2025-report.pdf (score: 2.74)\n",
      "   matching.    Early results show job seekers using Career Scout find and apply to relevant  jobs 7x faster and are 38% more likely to be hired, with 84...\n",
      "3. the-state-of-enterprise-ai_2025-report.pdf (score: 2.68)\n",
      "   symptom-related questions,  preparing for visits, and explaining follow-up instructions, while also escalating  members to providers or care guides as...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example searches\n",
    "queries = [\n",
    "    \"What are the key trends in artificial intelligence?\",\n",
    "    \"How is AI impacting healthcare?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, r in enumerate(hybrid_search(query, top=3), 1):\n",
    "        print(f\"{i}. {r['title']} (score: {r.get('@search.reranker_score', 'N/A'):.2f})\")\n",
    "        print(f\"   {r['chunk'][:150].replace(chr(10), ' ')}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete all resources\n",
    "# for name, delete_fn in [(indexer_name, indexer_client.delete_indexer),\n",
    "#                         (skillset_name, indexer_client.delete_skillset),\n",
    "#                         (data_source_name, indexer_client.delete_data_source_connection),\n",
    "#                         (index_name, index_client.delete_index)]:\n",
    "#     delete_fn(name); print(f\"Deleted: {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
