{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing storage usage for vectors on Azure AI Search\n",
    "\n",
    "This code demonstrates how to use the following features to reduce vector storage on Azure AI Search.\n",
    "\n",
    "+ Use smaller \"narrow\" data types instead of `Edm.Single`. Types such as `Edm.Half` reduce storage overhead.\n",
    "+ Disable storing vectors used in the query response. Vectors returned in a query response are stored separately from the vectors used during queries.\n",
    "+ Quantizing vectors. Use built-in scalar or binary quantization to quantize embeddings to `Edm.Int8` without any reduction in query performance. Information loss from quantization can be compensated for using the original unquantized embeddings and oversampling.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "+ An Azure subscription.\n",
    " \n",
    "+ Azure AI Search, any tier, but we recommend Basic or higher for this workload. [Enable semantic ranker](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable) if you want to run a hybrid query with semantic ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r vector-quantization-and-storage-requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .env file (Copy .env-sample to .env and update accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\", \"\")) if len(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\", \"\")) > 0 else DefaultAzureCredential()\n",
    "base_index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"teststorage\")\n",
    "embedding_dimensions = int(os.getenv(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", 3072))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings\n",
    "\n",
    "Load the embeddings from a precomputed file. These embeddings use [text-embedding-3-large](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#embeddings) with 3072 dimensions. The chunks are from the sample data in the document folder, chunked using the [Split Skill](https://learn.microsoft.com/azure/search/cognitive-search-skill-textsplit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lib.embeddings import content_path\n",
    "\n",
    "with open(content_path, \"r\") as f:\n",
    "    chunks = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create indexes\n",
    "\n",
    "To demonstrate the storage impact of the different options, the following code creates indexes that use each option, and another index that combines all the options together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the indexes on the search service\n",
    "from typing import List\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    ScalarQuantizationCompression,\n",
    "    BinaryQuantizationCompression,\n",
    "    VectorSearchCompression\n",
    ")\n",
    "\n",
    "def create_index(index_name, dimensions, use_scalar_compression=False, use_binary_compression=False, use_float16=False, use_stored=True):\n",
    "    if use_float16:\n",
    "        vector_type = \"Collection(Edm.Half)\"\n",
    "    else:\n",
    "        vector_type = \"Collection(Edm.Single)\"\n",
    "\n",
    "    # Vector fields that aren't stored can never be returned in the response\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True),\n",
    "        SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"chunk\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"embedding\", type=vector_type, searchable=True, stored=use_stored, vector_search_dimensions=dimensions, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]\n",
    "\n",
    "    compression_configurations: List[VectorSearchCompression] = []\n",
    "    if use_scalar_compression:\n",
    "        compression_name = \"myCompression\"\n",
    "        compression_configurations = [\n",
    "            ScalarQuantizationCompression(compression_name=compression_name)\n",
    "        ]\n",
    "    elif use_binary_compression:\n",
    "        compression_name = \"myCompression\"\n",
    "        compression_configurations = [\n",
    "            BinaryQuantizationCompression(compression_name=compression_name)\n",
    "        ]\n",
    "    else:\n",
    "        compression_name = None\n",
    "        compression_configurations = []\n",
    "    \n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(name=\"myHnsw\")\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(name=\"myHnswProfile\", algorithm_configuration_name=\"myHnsw\", compression_name=compression_name)\n",
    "        ],\n",
    "        compressions=compression_configurations\n",
    "    )\n",
    "\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "        )\n",
    "    )\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    return SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created indexes\n"
     ]
    }
   ],
   "source": [
    "# Create indexes to compare storage usage\n",
    "# The baseline index does not use any options\n",
    "\n",
    "baseline_index = f\"{base_index_name}-baseline\"\n",
    "scalar_compression_index = f\"{base_index_name}-scalar-compression\"\n",
    "binary_compression_index = f\"{base_index_name}-binary-compression\"\n",
    "narrow_index = f\"{base_index_name}-narrow\"\n",
    "no_stored_index = f\"{base_index_name}-no-stored\"\n",
    "all_index_scalar = f\"{base_index_name}-all-options-with-scalar\"\n",
    "all_index_binary = f\"{base_index_name}-all-options-with-binary\"\n",
    "\n",
    "search_index_client = SearchIndexClient(endpoint, credential)\n",
    "search_index_client.create_or_update_index(\n",
    "    create_index(scalar_compression_index, embedding_dimensions, use_scalar_compression=True))\n",
    "search_index_client.create_or_update_index(\n",
    "    create_index(binary_compression_index, embedding_dimensions, use_binary_compression=True))\n",
    "search_index_client.create_or_update_index(\n",
    "    create_index(narrow_index, embedding_dimensions, use_float16=True))\n",
    "search_index_client.create_or_update_index(\n",
    "    create_index(no_stored_index, embedding_dimensions, use_stored=False))\n",
    "search_index_client.create_or_update_index(\n",
    "    create_index(all_index_scalar, embedding_dimensions, use_scalar_compression=True, use_float16=True, use_stored=False))\n",
    "search_index_client.create_or_update_index(\n",
    "    create_index(all_index_binary, embedding_dimensions, use_binary_compression=True, use_float16=True, use_stored=False))\n",
    "search_index_client.create_or_update_index(\n",
    "    create_index(baseline_index, embedding_dimensions))\n",
    "print(\"Created indexes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload the embeddings to each index\n",
    "\n",
    "import json\n",
    "from lib.embeddings import content_path\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "def upload_embeddings(index_name):\n",
    "    with open(content_path, \"r\") as f:\n",
    "        content = json.load(f)\n",
    "    \n",
    "    with SearchIndexingBufferedSender(endpoint, index_name, credential) as client:\n",
    "        client.upload_documents(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded embeddings\n"
     ]
    }
   ],
   "source": [
    "upload_embeddings(scalar_compression_index)\n",
    "upload_embeddings(binary_compression_index)\n",
    "upload_embeddings(narrow_index)\n",
    "upload_embeddings(no_stored_index)\n",
    "upload_embeddings(all_index_scalar)\n",
    "upload_embeddings(all_index_binary)\n",
    "upload_embeddings(baseline_index)\n",
    "\n",
    "print(\"Uploaded embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check storage sizes\n",
    "\n",
    "Find the new storage size in MB to demonstrate how the various options affect storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceNotFoundError",
     "evalue": "() An index named 'quantize-scalar-compression' could not be found.\nCode: \nMessage: An index named 'quantize-scalar-compression' could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     response \u001b[38;5;241m=\u001b[39m search_index_client\u001b[38;5;241m.\u001b[39mget_index_statistics(index_name)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bytes_to_mb(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]), bytes_to_mb(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_index_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 12\u001b[0m index_sizes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_storage_size_mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mscalar_compression_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_compression_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_stored_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnarrow_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_index_scalar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_index_binary\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m index_sizes\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((storage_size, vector_size), index_name) \u001b[38;5;129;01min\u001b[39;00m index_sizes:\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m     response \u001b[38;5;241m=\u001b[39m search_index_client\u001b[38;5;241m.\u001b[39mget_index_statistics(index_name)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bytes_to_mb(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]), bytes_to_mb(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_index_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 12\u001b[0m index_sizes \u001b[38;5;241m=\u001b[39m [(\u001b[43mfind_storage_size_mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m, index_name) \u001b[38;5;28;01mfor\u001b[39;00m index_name \u001b[38;5;129;01min\u001b[39;00m [scalar_compression_index, binary_compression_index, baseline_index, no_stored_index, narrow_index, all_index_scalar, all_index_binary]]\n\u001b[0;32m     13\u001b[0m index_sizes\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((storage_size, vector_size), index_name) \u001b[38;5;129;01min\u001b[39;00m index_sizes:\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36mfind_storage_size_mb\u001b[1;34m(index_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_storage_size_mb\u001b[39m(index_name):\n\u001b[1;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msearch_index_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bytes_to_mb(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]), bytes_to_mb(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_index_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\magottei\\source\\azure-search-vector-samples\\demo-python\\code\\vector-quantization-and-storage\\.venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\magottei\\source\\azure-search-vector-samples\\demo-python\\code\\vector-quantization-and-storage\\.venv\\Lib\\site-packages\\azure\\search\\documents\\indexes\\_search_index_client.py:171\u001b[0m, in \u001b[0;36mSearchIndexClient.get_index_statistics\u001b[1;34m(self, index_name, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns statistics for the given index, including a document count\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03mand storage usage.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_client_headers(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 171\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mas_dict()\n",
      "File \u001b[1;32mc:\\Users\\magottei\\source\\azure-search-vector-samples\\demo-python\\code\\vector-quantization-and-storage\\.venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\magottei\\source\\azure-search-vector-samples\\demo-python\\code\\vector-quantization-and-storage\\.venv\\Lib\\site-packages\\azure\\search\\documents\\indexes\\_generated\\operations\\_indexes_operations.py:913\u001b[0m, in \u001b[0;36mIndexesOperations.get_statistics\u001b[1;34m(self, index_name, request_options, **kwargs)\u001b[0m\n\u001b[0;32m    910\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m]:\n\u001b[1;32m--> 913\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n",
      "File \u001b[1;32mc:\\Users\\magottei\\source\\azure-search-vector-samples\\demo-python\\code\\vector-quantization-and-storage\\.venv\\Lib\\site-packages\\azure\\core\\exceptions.py:161\u001b[0m, in \u001b[0;36mmap_error\u001b[1;34m(status_code, response, error_map)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    160\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[1;31mResourceNotFoundError\u001b[0m: () An index named 'quantize-scalar-compression' could not be found.\nCode: \nMessage: An index named 'quantize-scalar-compression' could not be found."
     ]
    }
   ],
   "source": [
    "# Please note - there may be delays in finding index statistics after document upload\n",
    "# Index statistics is not a real time API\n",
    "# See https://learn.microsoft.com/en-us/rest/api/searchservice/preview-api/get-index-statistics for more information\n",
    "\n",
    "def bytes_to_mb(bytes):\n",
    "    return round(bytes / (1024 * 1024), 4)\n",
    "\n",
    "def find_storage_size_mb(index_name):\n",
    "    response = search_index_client.get_index_statistics(index_name)\n",
    "    return bytes_to_mb(response[\"storage_size\"]), bytes_to_mb(response[\"vector_index_size\"])\n",
    "\n",
    "index_sizes = [(find_storage_size_mb(index_name), index_name) for index_name in [scalar_compression_index, binary_compression_index, baseline_index, no_stored_index, narrow_index, all_index_scalar, all_index_binary]]\n",
    "index_sizes.sort(key=lambda item: item[0][0], reverse=True)\n",
    "\n",
    "for ((storage_size, vector_size), index_name) in index_sizes:\n",
    "    print(\"*\" * 40)\n",
    "    print(f\"Index Name: {index_name}\\nStorage Size: {storage_size}MB\\nVector Size: {vector_size}MB\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
