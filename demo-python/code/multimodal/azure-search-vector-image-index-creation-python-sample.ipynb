{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal on Azure AI Search using Azure AI Vision\n",
    "\n",
    "This code demonstrates a no-indexer approach to vectorization of image content and queries on Azure AI Search.\n",
    "\n",
    "It uses [Azure AI Vision 4.0](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/image-retrieval) for multimodal embeddings. You must have a [Computer Vision resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesComputerVision) to get your key and endpoint. Be sure to create the resource in one of the permitted geographic regions: East US, France Central, Korea Central, North Europe, Southeast Asia, West Europe, West US. You can use the free tier to run this demo.\n",
    "\n",
    "In constrast with the [multimodal-custom-skill example](https://github.com/Azure/azure-search-vector-samples/tree/main/demo-python/code/multimodal-custom-skill) that uses indexers and skills for Azure AI Vision image vectorization, this example uses the [push approach](https://learn.microsoft.com/azure/search/search-what-is-data-import) to indexing.\n",
    "\n",
    "+ Create an embeddings instance\n",
    "+ Load and vectorize image files from a local folder\n",
    "+ Create an index\n",
    "+ Push vectorized image content to the index\n",
    "+ Run image queries\n",
    "\n",
    "This is a pure image search scenario, with no text input from users. For an example of the multimodal embeddings over text search, see the [multimodal-custom-skill example](https://github.com/Azure/azure-search-vector-samples/tree/main/demo-python/code/multimodal-custom-skill)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r azure-search-vector-image-index-creation-python-sample-requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .env file\n",
    "\n",
    "Copy `.env-sample` to an `.env` file in the sample folder, and update accordingly. The search service and Azure AI Vision endpoint must exist, but the search index is created and loaded during code execution. Provide a unique name for the index and make sure your search service has sufficient quota. Endpoints and API keys can be found in the Azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "vision_key = os.environ[\"AZURE_AI_VISION_KEY\"]\n",
    "vision_endpoint = os.environ[\"AZURE_AI_VISION_ENDPOINT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create image embedding\n",
    "\n",
    "Sets up an embedding instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tenacity import (\n",
    "    Retrying,\n",
    "    retry_if_exception_type,\n",
    "    wait_random_exponential,\n",
    "    stop_after_attempt\n",
    ")\n",
    "import json\n",
    "import mimetypes\n",
    "\n",
    "params = {  \n",
    "        \"api-version\": \"2023-02-01-preview\",\n",
    "        \"overload\": \"stream\",\n",
    "        \"modelVersion\": \"latest\"\n",
    "}\n",
    "url = f\"{vision_endpoint}/computervision/retrieval:vectorizeImage\"  \n",
    "\n",
    "def vectorize_image(image_path):\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    headers = {  \n",
    "        \"Content-Type\": mime_type,\n",
    "        \"Ocp-Apim-Subscription-Key\": vision_key  \n",
    "    }\n",
    "    for attempt in Retrying(\n",
    "        retry=retry_if_exception_type(requests.HTTPError),\n",
    "        wait=wait_random_exponential(min=15, max=60),\n",
    "        stop=stop_after_attempt(15)\n",
    "    ):\n",
    "        with attempt:\n",
    "            with open(image_path, 'rb') as image_data:\n",
    "                response = requests.post(url, params=params, headers=headers, data=image_data)  \n",
    "                if response.status_code != 200:  \n",
    "                    response.raise_for_status()\n",
    "    vector = response.json()[\"vector\"]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed images\n",
    "\n",
    "Vectorize all the images and save them into a [json lines](https://jsonlines.org/) file at `\\data\\images\\apples\\output.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apples_image_directory = os.path.join('..', '..', 'data', 'images', 'apples')\n",
    "input_json_file = os.path.join(apples_image_directory, 'input.json')\n",
    "output_json_file = os.path.join(apples_image_directory, 'output.jsonl')\n",
    "with open(input_json_file, 'r') as infile:\n",
    "    images = json.load(infile)\n",
    "\n",
    "with open(output_json_file, 'w') as outfile:\n",
    "    for idx, image_data in enumerate(images):\n",
    "        image_path = image_data.get('image_path', None)\n",
    "        if image_path:\n",
    "            try:\n",
    "                vector = vectorize_image(os.path.join(apples_image_directory, image_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image at index {idx}: {e}\")\n",
    "                vector = None\n",
    "            \n",
    "            filename, _ = os.path.splitext(os.path.basename(image_path))\n",
    "            result = {\n",
    "                \"id\": f'{idx}',\n",
    "                \"image_vector\": vector,\n",
    "                \"description\": filename\n",
    "            }\n",
    "\n",
    "            outfile.write(json.dumps(result))\n",
    "            outfile.write('\\n')\n",
    "            outfile.flush()\n",
    "\n",
    "print(f\"Results are saved to {output_json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index\n",
    "\n",
    "Create a search index schema and vector search configuration on Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "# Create a search index \n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)  \n",
    "fields = [  \n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),  \n",
    "    SearchField(name=\"description\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),  \n",
    "    SearchField(\n",
    "        name=\"image_vector\",  \n",
    "        hidden=True,\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1024,  \n",
    "        vector_search_profile_name=\"myHnswProfile\"\n",
    "    ),  \n",
    "]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(  \n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],  \n",
    "   profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "        )\n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "# Create the search index with the vector search configuration  \n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push local data to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "import json\n",
    "\n",
    "apples_image_directory = os.path.join('..', '..', 'data', 'images', 'apples')\n",
    "output_json_file = os.path.join(apples_image_directory, 'output.jsonl')\n",
    "\n",
    "data = []\n",
    "with open(output_json_file, 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove leading/trailing whitespace and parse JSON\n",
    "        json_data = json.loads(line.strip())\n",
    "        data.append(json_data)\n",
    "\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "results = search_client.upload_documents(data)\n",
    "for result in results:\n",
    "    print(f'Indexed {result.key} with status code {result.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform an image vector search\n",
    "\n",
    "Perform a vector search to find the most relevant images based on an image query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "from IPython.display import Image\n",
    "\n",
    "# Generate text embeddings for the query  \n",
    "query_image_path = os.path.join(apples_image_directory, \"two_apples.jpeg\")\n",
    "vector_query = VectorizedQuery(vector=vectorize_image(query_image_path),\n",
    "                              k_nearest_neighbors=2, \n",
    "                              fields=\"image_vector\")  \n",
    "\n",
    "# Perform vector search  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"description\"],\n",
    "    top=2\n",
    ")   \n",
    "  \n",
    "# Print the search results \n",
    "print(f\"Search results for {query_image_path}:\") \n",
    "display(Image(query_image_path))\n",
    "for result in results:\n",
    "    print(f\"Apple Type: {result['description']}\")\n",
    "    print(\"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
