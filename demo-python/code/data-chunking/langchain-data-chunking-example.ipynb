{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain data chunking example\n",
    "\n",
    "This notebook uses Langchain's recursive character text splitter to chunk text. Source files are large PDFs loaded using PyPDFLoader.\n",
    "\n",
    "The notebook complements the [Chunking large documents for vector search solutions](https://learn.microsoft.com/azure/search/vector-search-how-to-chunk-document) article in the Azure AI Search documentation.\n",
    "\n",
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .env file (Copy .env-sample to .env and update accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "search_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_embedding_deployment_id = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_ID\"]\n",
    "recursivetextsplitter_searchindex = os.environ[\"AZURE_SEARCH_LANGCHAIN_RECURSIVETEXTSPLITTER_INDEX\"]\n",
    "\n",
    "search_credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"] if len(os.environ[\"AZURE_OPENAI_KEY\"]) > 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup sample resources for embedding chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import get_bearer_token_provider\n",
    "\n",
    "azure_openai_client = None\n",
    "if azure_openai_key:\n",
    "    azure_openai_client = AzureOpenAI(\n",
    "        api_key=azure_openai_key, \n",
    "        api_version=\"2023-05-15\",\n",
    "        azure_deployment=azure_openai_embedding_deployment_id,\n",
    "        azure_endpoint=azure_openai_endpoint)\n",
    "else:\n",
    "    azure_openai_client = AzureOpenAI(\n",
    "        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n",
    "        api_version=\"2023-05-15\",\n",
    "        azure_deployment=azure_openai_embedding_deployment_id,\n",
    "        azure_endpoint=azure_openai_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup sample resources for recursive text splitter chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from lib.common import (\n",
    "    create_search_index,\n",
    ")\n",
    "\n",
    "search_index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "rts_searchindex = create_search_index(\n",
    "    recursivetextsplitter_searchindex,\n",
    "    azure_openai_endpoint,\n",
    "    azure_openai_embedding_deployment_id,\n",
    "    azure_openai_key\n",
    ")\n",
    "search_index_client.create_or_update_index(rts_searchindex)\n",
    "\n",
    "print(\"Created recursive text splitter index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "loader = PyPDFLoader(os.path.join(\"data\", \"earth_at_night_508.pdf\"))\n",
    "pages = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate histogram of token and character lengths per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from lib.common import plot_chunk_histogram, get_token_length\n",
    "\n",
    "page_content = [page.page_content for page in pages]\n",
    "\n",
    "plot_chunk_histogram(\n",
    "    chunks=page_content,\n",
    "    length_fn=len,\n",
    "    title=\"Distribution of page character lengths\",\n",
    "    xlabel=\"Page character length\",\n",
    "    ylabel=\"Page count\")\n",
    "plot_chunk_histogram(\n",
    "    chunks=page_content,\n",
    "    length_fn=get_token_length,\n",
    "    title=\"Distribution of page token lengths\",\n",
    "    xlabel=\"Page token length\",\n",
    "    ylabel=\"Page count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk PDF using Recursive text splitter\n",
    "\n",
    "We use the output of the above historgrams to guide us into selecting a 600 token chunk length with a 150 token overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from lib.common import get_encoding_name\n",
    "\n",
    "# from_tiktoken_encoder enables use to split on tokens rather than characters\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "   encoding_name=get_encoding_name(),\n",
    "   chunk_size=600, \n",
    "   chunk_overlap=125\n",
    ")\n",
    "\n",
    "recursive_text_splitter_chunks = recursive_text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate histogram of chunk character and token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_content = [chunk.page_content for chunk in recursive_text_splitter_chunks]\n",
    "\n",
    "plot_chunk_histogram(\n",
    "    chunks=chunk_content,\n",
    "    length_fn=len,\n",
    "    title=\"Distribution of chunk character lengths\",\n",
    "    xlabel=\"Chunk character length\")\n",
    "plot_chunk_histogram(\n",
    "    chunks=chunk_content,\n",
    "    length_fn=get_token_length,\n",
    "    title=\"Distribution of chunk token lengths\",\n",
    "    xlabel=\"Chunk token length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Recursive text splitter chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_text_splitter_embeddings = azure_openai_client.embeddings.create(input=chunk_content, model=azure_openai_embedding_deployment_id)\n",
    "recursive_text_splitter_embeddings = [result.embedding for result in recursive_text_splitter_embeddings.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload chunks to search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_search_client = search_index_client.get_search_client(recursivetextsplitter_searchindex)\n",
    "\n",
    "docs = [\n",
    "    {\n",
    "        \"parent_id\": \"0\",\n",
    "        \"chunk_id\": f\"earth-at-night-508-pdf_0_0_{i}\",\n",
    "        \"chunk\": chunk.page_content,\n",
    "        \"title\": \"earth_at_night_508.pdf\",\n",
    "        \"vector\": recursive_text_splitter_embeddings[i]\n",
    "    }\n",
    "    for i, chunk in enumerate(recursive_text_splitter_chunks)\n",
    "]\n",
    "\n",
    "recursive_search_client.upload_documents(docs)\n",
    "\n",
    "print(\"Uploaded chunks and embeddings for recursive text splitter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
